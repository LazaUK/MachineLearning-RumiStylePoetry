{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text in Rumi-style, by training NLP model on Rumi poetry dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-generated cell from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/rumi_poetry.xlsx\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import tensorflow as tf # Deep Learning framework\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./datasets/rumi_poetry.xlsx'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting source file name / path\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "source_file = filenames[0]\n",
    "source_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown Existence</td>\n",
       "      <td>unknown existence\\nundiscovered beauty\\nthat's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In This Earth</td>\n",
       "      <td>in this earth\\nin this earth\\nin this immacula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All The Precious Words</td>\n",
       "      <td>all the precious words\\nyou and i have exchang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Sweetheart</td>\n",
       "      <td>the sweetheart\\nwho is blocking my sleep\\ndema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I Know of a Path in Your Heart</td>\n",
       "      <td>i know of a path in your heart\\nthat merges wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Come</td>\n",
       "      <td>come \\nwhoever you are\\ncome\\ncome again\\neven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>I Am No Lion</td>\n",
       "      <td>i am no lion \\nto overpower my enemies\\nwinnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Restless</td>\n",
       "      <td>restless\\nnow i go to the door\\nnow i go on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>All My Friends</td>\n",
       "      <td>all my friends\\ndeparted like dreams\\nleft alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Believe Me</td>\n",
       "      <td>believe me\\ni wasn't always like this\\nlacking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Title  \\\n",
       "0                Unknown Existence   \n",
       "1                    In This Earth   \n",
       "2           All The Precious Words   \n",
       "3                   The Sweetheart   \n",
       "4   I Know of a Path in Your Heart   \n",
       "..                             ...   \n",
       "83                            Come   \n",
       "84                    I Am No Lion   \n",
       "85                        Restless   \n",
       "86                  All My Friends   \n",
       "87                      Believe Me   \n",
       "\n",
       "                                                 Poem  \n",
       "0   unknown existence\\nundiscovered beauty\\nthat's...  \n",
       "1   in this earth\\nin this earth\\nin this immacula...  \n",
       "2   all the precious words\\nyou and i have exchang...  \n",
       "3   the sweetheart\\nwho is blocking my sleep\\ndema...  \n",
       "4   i know of a path in your heart\\nthat merges wi...  \n",
       "..                                                ...  \n",
       "83  come \\nwhoever you are\\ncome\\ncome again\\neven...  \n",
       "84  i am no lion \\nto overpower my enemies\\nwinnin...  \n",
       "85  restless\\nnow i go to the door\\nnow i go on th...  \n",
       "86  all my friends\\ndeparted like dreams\\nleft alo...  \n",
       "87  believe me\\ni wasn't always like this\\nlacking...  \n",
       "\n",
       "[88 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating data source DataFrame\n",
    "source_df = pd.read_excel(source_file)\n",
    "source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original TensorFlow documentation sample on \"Text Generation with an RNN\":\n",
    "https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 64, 44, 32, 32, 22, 11, 53, 18, 11, 60, 64, 17, 32, 47, 55, 38,\n",
       "       20, 17, 57, 11, 26, 63, 10, 58,  5, 32, 43, 35, 43, 12, 23, 23, 40,\n",
       "        6, 12, 22, 55,  7, 29, 64,  7, 37, 58, 10, 18, 10, 17, 34,  8, 27,\n",
       "       30,  7, 52, 51,  2, 50,  7, 65,  3, 38, 44, 14, 18, 40, 52, 16, 19,\n",
       "       21, 41, 36, 60,  1,  7,  6, 61, 12, 63,  3, 49, 11, 25, 63, 45, 53,\n",
       "       53,  2, 12, 61, 60, 64, 45, 60,  7, 40, 30,  0, 57, 47,  4],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'uncle, do you know the cause?\\n\\nMONTAGUE:\\nI neither know it nor can learn of him.\\n\\nBENVOLIO:\\nHave you'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"WyeSSI:nE:uyDShpYGDr:Mx3s&SdVd;JJa';Ip,Py,Xs3E3DU-NQ,ml k,z!YeAEamCFHbWu\\n,'v;x!j:Lxfnn ;vuyfu,aQ[UNK]rh$\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1902356, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.038345"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 733s 4s/step - loss: 2.7401\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 6898s 40s/step - loss: 2.0040\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 470s 3s/step - loss: 1.7245\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 588s 3s/step - loss: 1.5591\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 707s 4s/step - loss: 1.4575\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 718s 4s/step - loss: 1.3867\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 583s 3s/step - loss: 1.3333\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 311s 2s/step - loss: 1.2882\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 324s 2s/step - loss: 1.2466\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 325s 2s/step - loss: 1.2066\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 325s 2s/step - loss: 1.1671\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 330s 2s/step - loss: 1.1261\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 318s 2s/step - loss: 1.0824\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 357s 2s/step - loss: 1.0379\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 931s 5s/step - loss: 0.9896\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 326s 2s/step - loss: 0.9397\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 394s 2s/step - loss: 0.8882\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 326s 2s/step - loss: 0.8370\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 310s 2s/step - loss: 0.7858\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 351s 2s/step - loss: 0.7371\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The world yield me; go and he smullows:\n",
      "This side is bloody disingrawel;\n",
      "But spoil you seek to many greatest strength;\n",
      "But show'd me hate: at laugh at thee?\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "I callument she lies yet.\n",
      "\n",
      "BRUTUS:\n",
      "Mark'd do yourself these words; the unjusticient malice\n",
      "may's absence more a fool thou art a\n",
      "proclained blood, none fit out with victorious wreads;\n",
      "Resign thy catter's worship, would not but\n",
      "the shape, nor waults upon the feather and that\n",
      "Romeo.\n",
      "\n",
      "MARIANA:\n",
      "Because six on that name; yea, one word I am tail\n",
      "Ty to his captain through a thorny wood,\n",
      "Whilst thou not slander with a habelest\n",
      "slepy.\n",
      "\n",
      "Servant:\n",
      "God save him, Angelo!\n",
      "\n",
      "ARIEL:\n",
      "The god of sailors shall were butts, being so hething.\n",
      "\n",
      "EDWARD:\n",
      "O pity, sir, that you may.\n",
      "Can she had come to lay myself as you.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Be gone:\n",
      "Tyrant your state great rootent. I will but\n",
      "With all your painings. Romeo hast not mack'd thee\n",
      "From those which I must give.\n",
      "\n",
      "ELWO:\n",
      "True, I think, if you be he not.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "For every day so h \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 5.1502392292022705\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThis doth not string from thee: thou do abott\\nMine easier the dee--trow down,\\nMust by the rock of strange, no lady-pain,\\nBecause since with Juliet is prey, stands.\\n\\nHERMIONE:\\nNo, I can read.\\n\\nSecond Citizen:\\nYour will, sir, what dost thou not shame,\\nWhich being castain, like lambs; he begs'd to Bolingbroke\\nA nabil up to hear at his command.\\n\\nPOLIXENES:\\nAy, but good morrow, gentle met me.\\n\\nPOMPEY:\\nI told you for your day, and all report that good line\\nBeing queen, your subjects. You, she wills,\\nI would not, that's a tale stomens.\\n\\nESCALUS:\\nIt is the enfolding I swore oracle\\nI mean to do so; you show your flesh and breath\\nAs when they shall succorder them as we place.\\n\\nPOLIXENES:\\nO, 'tis a good speedy well! Come along?\\n\\nDUKE OF YORK:\\nIt would inconstant thee, from The relatit ere\\nA mile of such a name of moen inclence\\nBefore he show'd our conquest did.\\n\\nNurse:\\nThis is a heavy complace: come I may not say no.\\n\\nVOLUMNIA:\\nThou hast your deed, not four--to be a charge\\nMore than I was a bagg\"\n",
      " b\"ROMEO:\\nI told you, sir.\\n\\nSeephere:\\nAy, ay, what word, as too young force.\\n\\nQUEEN ELIZABETH:\\nCousin of Hereford, you must go to grave;\\nTo work to take from thine arms.\\nCome, Romeo, bring both thy abold gentlemen,\\nTo bear for Rome to England's queen.\\nFar be the point to bear, and all thy purest\\nUntilly of no posterity, a bawd, a\\njetted for no ground;\\nMethinks I see a way should do attempt him:\\nAnd what you will, like one los'd,\\nI have well placed the frand's ears whire;\\nI lay it on.\\n\\nAUFIDIUS:\\nGo, get a heaven uncontinent!\\nExformed such a hops of death,\\nBut the worm and fully discover i' the air,\\nBut shall the Duke of Gloucester Froth, awake\\nMy belly earth, the hard, which else thus faults.\\n\\nGLOUCESTER:\\nWouldst gland\\nA fatch amill up to sea and land of--\\nThis hour is hath to reap the way;\\nI never yet revell upon an infirect,\\nSo long have more than the Lord Huckntir\\nWilt thou scarce have pardon up,\\nUnlessortain leaves the supposed-king,\\nAnd bid her down the lives my peace to death.\\nNeast, shall\"\n",
      " b\"ROMEO:\\nI have so, speed all good against my sister.\\n\\nGRUMIO:\\nAmen, amen. God save the Copitulal I spoke of;\\nForbiddy, gods, nor me, so him, I have it dear\\nAs every man as we as leaving.\\n\\nQUEEN MARGARET:\\nPeace is a word: all's for my castle.\\n\\nThird Servingman:\\nUnder the sand do prink shall go about that burning glory.\\nThough five had private a drink. He says he malice.\\n\\nRIVERS:\\nHis paper hath revolus in your time:\\nI have said you will be dogg'd still.\\n\\nESCALUS:\\nThat you, Tybalt, yet Blind Citizens.\\n\\nCAMILLO:\\nGood my loyal lord!\\nTo gree in thirst unwinging open of yours.\\n\\nBENVOLIO:\\nThe date hath been by so chastisement:\\nI'll hear no nature. These height\\nBe ready in my hands, and thine shake piny.\\n\\nBAPTISTA:\\nAy. The banks it was so mine own tritan\\nMost affaids she before what way,\\nThrice bloody mind, honest nurse, which thou shalt alter it,\\nWhen he courished in the commonwealth for\\nHethings should be past company: our garments-brook doorme\\nWithout thee: if it be,\\nThe smallest willow gait away?\\n\"\n",
      " b\"ROMEO:\\n\\nALONSO:\\nI should leave me to want the king;\\nAnd with thy heart for ever wild hath\\nHer ripel'd thunder or Crittly not.\\n\\nGONZALO:\\nAll her father, be not so deal and speed him well\\nAnd live with many thousands winds,\\nNeighbourness' heaviers dangerously, govern'd love;\\nRescoinged me with mercy. You're a plague place.\\n\\nLADY CAPULET:\\nWe are 'dost thou the matter?\\n\\nDUKE VINCENTIO:\\nAlas, your honour means to make it.\\n\\nShepherd:\\nMy govimes, ho!\\n\\nhest one, another way.\\n\\nLEONTES:\\nA girl\\ngood sister, Pace. How swear\\nLessee in him. You are not peace\\nMy things and milfly; fair and virtuous!\\n\\nCATESBY:\\nHe is engine your storm?\\n\\nAUFIDIUS:\\nO'll be your comfort.\\n\\nELBOW:\\nMarry, so I reap with you. How now!\\nFortune will I say, in any proper land.\\nCourdefur our general me nay, you have\\npauden dine and says 'God sever from the shiel.\\n\\nAUTOLYCUS:\\nO, heaven! I do seek how playal\\nThe child, in speaks that heaven meat way;\\nThough craple-bed, whom I prosper your years,\\nI would do there some present ates your st\"\n",
      " b\"ROMEO:\\nHe shall not speak this here.\\n\\nCLARENCE:\\nWe would say yond gracious lord: I carry it your titte\\nof us: and it have the prince for God's face,\\nA rastle of the house of York suffer no other\\nTo troop and swear: so witch courtesy,\\nWhen blow the Duke of Clarence say, who other swift,\\nHis currish walls: wise, and stands for our defence:\\nMy faith resolved! most wancing share the\\nvewbroke! I say; sweet Clarence gid!\\n\\nRATCLIFF:\\nMy lord of WeStman is gone before strike upon my grief;\\nJold, after, I warrant thee, thou shalt go see my heart\\nBe undertaining 'Good of Grounds, thou speak'st,\\nThough not a fairer bend, they would not stay.\\n\\nHENRY BOLINGBROKE:\\nNay, hear you to my heart when I amilf a king.\\n\\nKING HENRY VI:\\nPray you, sir, it is so, break or; therefore bark,\\nI wind a man that he I may part few\\nFor such a one amongeth it eagly wondering,\\nThe needlewn part with rain: yet they account a\\nrobet, henceforward starves and mine,\\nI show his innocent soul exceed the governary!\\nHow slew your honour \"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 5.304223299026489\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000027C88480CA0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The doom of this is sin under thy tale\n",
      "Again to prison. Good! get thee gone,\n",
      "Which seizest thou fox\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8230a386d8e0083990873cddb8ebb5b6213275a10339230a8504f0ef8ce7f888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
